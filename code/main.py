import rooms
import agent as a
import matplotlib.pyplot as plot
import sys
import math

def episode(env, agent, nr_episode=0, q_value=None, evaluation=True):
    state = env.reset()
    discounted_return = 0
    discount_factor = params['gamma']
    done = False
    time_step = 0
    temp = {}
    while not done:
        # 1. Select action according to policy
        action = agent.policy(state, evaluation)
        # 2. Execute selected action
        next_state, reward, terminated, truncated, _ = env.step(action)
        # 3. Integrate new experience into agent
        if not evaluation:
            if nr_episode == 0:
                temp = agent.update(state, action, reward, next_state, terminated, truncated, q_value)
            else:
                temp = agent.update(state, action, reward, next_state, terminated, truncated)
        state = next_state
        done = terminated or truncated
        discounted_return += (discount_factor**time_step)*reward
        time_step += 1
    print(nr_episode, ":", discounted_return)
    return (temp,discounted_return)

def train_test_func(no_episodes, agent, test=False, temp=None):
    '''
     Running the episodes for training and testing phase
    '''
    temp_val = {}
    if test == False:
        returns = list()
        for i in range(no_episodes):
            temp_val, return_temp = episode(env, agent, i, evaluation=False)
            returns.append(return_temp)
        return (temp_val, returns) 
    else:
        returns = list()
        for i in range(no_episodes):
            if i == 0:
                temp_val, return_temp = episode(env, agent, i,  temp)
                returns.append(return_temp)
            else:
                temp_val, return_temp = episode(env, agent, i)
                returns.append(return_temp)
        return returns
    

def plot_func(no_episodes, returns):
    '''
     Plotting the returns generated by an agent against the number of episodes
    '''
    x = range(no_episodes)
    y = returns
    plot.plot(x,y)
    plot.title("Progress")
    plot.xlabel("Episode")
    plot.ylabel("Discounted Return")
    plot.show()
    plot.close()

def plot_compares(no_episodes, returns1, returns2, returns3):
    '''
     Plotting the returns generated by an agent against the number of episodes for multiple bandit comparison
    '''
    x = range(no_episodes)
    y1, y2, y3 = returns1, returns2, returns3
    plot.plot(x,y1, label = 'Epsilon Greedy Bandit', alpha = 0.5)
    plot.plot(x,y2, label = 'UCB1 Bandit', alpha = 0.5)
    plot.plot(x,y3, label = 'Boltzmann Bandit', alpha = 0.5)
    plot.legend()
    plot.title("Progress")
    plot.xlabel("Episode")
    plot.ylabel("Discounted Return")
    plot.show()
    plot.close()


params = {}
rooms_instance = sys.argv[1]
choice = int(input('Press 1 to see the performance of Q Learning agent with Epsilon-Greedy bandit.\nPress 2 to see the performance of Q Learning agent with UCB1 bandit.\nPress 3 to see the performance of Q Learning agent with Boltzmann bandit.\nPress 4 to compare the performance of Q Learning agent with above mentioned bandit.\n'))
env = rooms.load_env(f"layouts/{rooms_instance}.txt", f"{rooms_instance}.mp4")
params["nr_actions"] = env.action_space.n
params["gamma"] = 0.997 
params["epsilon_decay"] = 0.001 #0.00001 # Epsilon value for Epsilon Greedy Algorithm
params["alpha"] = 0.1 # Learning Rate
params["env"] = env
params['epsilon'] = 0.9 # Epsilon Value
params['exploration_constant'] = math.sqrt(2) # Exploration Constant for UCB1 bandit
params['temperature'] = 10 # Temperature value for Boltzmann Bandit
params['exploration_decay'] = 0.001
params['temperature_decay'] = 0.01
training_episodes = 500
testing_episodes = 10

# For all Q-Learning algorithms
if choice == 1: # Epsilon Greedy 
    agent = a.QLearner_EpsilonGreedy(params)
    temp, returns = train_test_func(training_episodes, agent)
    plot_func(training_episodes, returns)
    returns_test = train_test_func(testing_episodes, agent, True, temp)
    plot_func(testing_episodes, returns_test)
    env.save_video()
elif choice == 2: # UCB 1
    agent = a.QLearner_UCB1(params)
    temp, returns = train_test_func(training_episodes, agent)
    plot_func(training_episodes, returns)
    returns_test = train_test_func(testing_episodes, agent, True, temp)
    plot_func(testing_episodes, returns_test)
    env.save_video()
elif choice == 3: # Boltzmann
    agent = a.QLearner_Boltzmann(params)
    temp, returns = train_test_func(training_episodes, agent)
    plot_func(training_episodes, returns)
    returns_test = train_test_func(testing_episodes, agent, True, temp)
    plot_func(testing_episodes, returns_test)
    env.save_video()
elif choice == 4: # Comparing all three
    agent1 = a.QLearner_EpsilonGreedy(params)
    temp1, returns1 = train_test_func(training_episodes, agent1)
    returns_test1 = train_test_func(testing_episodes, agent1, True, temp1)
    agent2 = a.QLearner_UCB1(params)
    temp2, returns2 = train_test_func(training_episodes, agent2)
    returns_test2 = train_test_func(testing_episodes, agent2, True, temp2)
    agent3 = a.QLearner_Boltzmann(params)
    temp3, returns3 = train_test_func(training_episodes, agent3)
    returns_test3 = train_test_func(testing_episodes, agent3, True, temp3)
    plot_compares(training_episodes, returns1, returns2, returns3)
    plot_compares(testing_episodes, returns_test1, returns_test2, returns_test3)

    

    

